{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import data\n",
    "\n",
    "#import main data\n",
    "folder = r'\\\\smc-filer\\divsede\\APPS\\EVA\\notebooks\\notebooks\\users\\Hani_Yacoub\\\\'\n",
    "name = 'data2021.csv'\n",
    "df = pd.read_csv(folder + name)\n",
    "df = df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "branches = [9,12,1,10,2]\n",
    "products = [31,21,31,48,1,29]\n",
    "\n",
    "df = df[df['branch_code'].isin(branches)]\n",
    "df = df[df['item_cat'].isin(products)]\n",
    "\n",
    "df = df.reset_index().drop([\"index\"],axis=1)\n",
    "\n",
    "#create data for all, and just for 2019 to test it\n",
    "\n",
    "df_2019 = df[df[\"year\"]==2019]\n",
    "\n",
    "df = df[df[\"year\"]!=2019]\n",
    "\n",
    "\n",
    "# make a prediction with a stacking ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "X = df.drop([\"sales\",\"ym\"],axis=1)\n",
    "\n",
    "y = df[[\"sales\"]]\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_test_orginal = X_test\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['branch_code', 'item_cat', 'promotion', 'm'])\n",
    "X_test = pd.get_dummies(X_test, columns=['branch_code', 'item_cat', 'promotion', 'm'])\n",
    "\n",
    "# define the base models\n",
    "level0 = list()\n",
    "level0.append(('knn', KNeighborsRegressor(n_neighbors=10)))\n",
    "level0.append(('cart', DecisionTreeRegressor()))\n",
    "level0.append(('RF', RandomForestRegressor(n_estimators=100,max_features='log2', verbose=1)))\n",
    "level0.append(('svm', SVR()))\n",
    "level0.append(('nn', MLPRegressor(random_state=1, max_iter=500)))\n",
    "\n",
    "# define meta learner model\n",
    "level1 = LinearRegression()\n",
    "# define the stacking ensemble\n",
    "model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n",
    "# fit the model on all available data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make a prediction for one example\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(r2_score(y_test, yhat))\n",
    "\n",
    "#we need to see how our model did on unseen data that is in the future, to see its forcasting power\n",
    "\n",
    "#we take the validation dataset which is in the year 2019, and we take the month of Feburary\n",
    "\n",
    "feb = df_2019[df_2019[\"m\"]==2]\n",
    "\n",
    "#keep the oringal dataset to make it easz to tranform it back from the get_dummies\n",
    "feb_or = feb\n",
    "\n",
    "#encode the categorlical data\n",
    "feb = pd.get_dummies(feb, columns=['branch_code', 'item_cat', 'promotion', 'm'])\n",
    "\n",
    "#drop uneeded columns\n",
    "feb2 = feb.drop([\"ym\",\"sales\"],axis=1)\n",
    "\n",
    "#enter missing columns\n",
    "for i in list(set(X_test.columns).difference(feb2.columns)):\n",
    "    \n",
    "    feb2[i] = 0\n",
    "\n",
    "#predict it using our Stacking model\n",
    "yhat = model.predict(feb2)\n",
    "\n",
    "#add our prediction to the dataset of Feburary\n",
    "feb_or[\"yhat\"] = yhat\n",
    "\n",
    "#lets see how good our model did\n",
    "print(r2_score(feb_or[\"sales\"], feb_or[\"yhat\"]))\n",
    "\n",
    "#lets plot the results. (pay attention the graph to understand it well, each point is an obserbvation in the month of Feb)\n",
    "feb_or.groupby([\"branch_code\",\"item_cat\",\"ym\"]).sum().plot(figsize=(20,20))\n",
    "\n",
    "\n",
    "#now we need to add the prediction of LSTM and the predictions of FACEBOOK prophet and see if we can get a better reuslts\n",
    "fbpred = []\n",
    "fbpred1 = []\n",
    "fbpred2 = []\n",
    "\n",
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "\n",
    "\n",
    "for i in list(df[\"branch_code\"].unique()):\n",
    "    for y in list(df[\"item_cat\"].unique()):\n",
    "        \n",
    "        x = df[(df[\"branch_code\"]==i)&(df[\"item_cat\"]==y)][[\"ym\",\"sales\"]].groupby(\"ym\").sum().reset_index().rename({\"ym\":\"ds\",\"sales\":\"y\"},axis=1)\n",
    "        \n",
    "        m = Prophet(seasonality_mode='multiplicative').fit(x)\n",
    "        future = m.make_future_dataframe(periods=3652)\n",
    "        \n",
    "        m = Prophet(seasonality_mode='multiplicative', mcmc_samples=300).fit(x)\n",
    "        fcst = m.predict(future)\n",
    "\n",
    "        future = m.make_future_dataframe(periods=120, freq='MS')\n",
    "        fcst = m.predict(future)\n",
    "        \n",
    "        num = max(0, int(fcst.set_index(\"ds\")[71:72][\"yhat\"]))\n",
    "        brnach_code1 = i\n",
    "        product_id = y\n",
    "        \n",
    "        \n",
    "        \n",
    "        fbpred.append(num)\n",
    "        fbpred1.append(brnach_code1)\n",
    "        fbpred2.append(product_id)\n",
    "        \n",
    "fp = pd.DataFrame()\n",
    "\n",
    "fp[\"pred\"] = fbpred\n",
    "fp[\"branch\"] = fbpred1\n",
    "fp[\"product\"] = fbpred2\n",
    "\n",
    "fp\n",
    "\n",
    "final1 = pd.merge(feb_or, fp,  how='left', left_on=['branch_code','item_cat'], right_on = ['branch','product'])\n",
    "final1.groupby([\"branch_code\",\"item_cat\",\"ym\"]).sum().plot(figsize=(20,20))\n",
    "\n",
    "\n",
    "#LSTM\n",
    "\n",
    "import pandas as pd\n",
    "#import data\n",
    "\n",
    "#import main data\n",
    "folder = r'\\\\smc-filer\\divsede\\APPS\\EVA\\notebooks\\notebooks\\users\\Hani_Yacoub\\\\'\n",
    "name = 'data2021.csv'\n",
    "df = pd.read_csv(folder + name)\n",
    "df = df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "branches = [9,12,1,10,2]\n",
    "products = [31,21,31,48,1,29]\n",
    "\n",
    "df = df[df['branch_code'].isin(branches)]\n",
    "df = df[df['item_cat'].isin(products)]\n",
    "\n",
    "df = df.reset_index().drop([\"index\"],axis=1)\n",
    "\n",
    "x = df[(df[\"branch_code\"]==2)&(df[\"item_cat\"]==21)][[\"ym\",\"sales\"]].groupby(\"ym\").sum().reset_index().rename({\"ym\":\"ds\",\"sales\":\"y\"},axis=1)\n",
    "\n",
    "x = x.set_index(\"ds\")\n",
    "x.index.freq = 'MS'\n",
    "\n",
    "\n",
    "#now we need to add the prediction of LSTM and the predictions of FACEBOOK prophet and see if we can get a better reuslts\n",
    "fbpred = []\n",
    "fbpred1 = []\n",
    "fbpred2 = []\n",
    "\n",
    "fp = pd.DataFrame()\n",
    "\n",
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "\n",
    "\n",
    "for i in list(df[\"branch_code\"].unique()):\n",
    "    for y in list(df[\"item_cat\"].unique()):\n",
    "        \n",
    "        print(i)\n",
    "        print(y)\n",
    "        \n",
    "        x = df[(df[\"branch_code\"]==i)&(df[\"item_cat\"]==y)][[\"ym\",\"sales\"]].groupby(\"ym\").sum().reset_index().rename({\"ym\":\"ds\",\"sales\":\"y\"},axis=1)\n",
    "        \n",
    "        x = x.set_index(\"ds\")\n",
    "        # fix random seed for reproducibility\n",
    "        seed = numpy.random.seed(7)\n",
    "\n",
    "        # normalize the dataset\n",
    "        data_range = (-1, 1)\n",
    "        scaler = MinMaxScaler(feature_range=data_range)        # scaler can also de-normalize the dataset by scaler.inverse_transform(), useful for actual prediction\n",
    "        dataset_scaled = scaler.fit_transform(x)\n",
    "        #dataset_scaled = numpy.array(dataset_scaled)\n",
    "\n",
    "        # split into train and test sets\n",
    "        train_size = int(len(dataset_scaled) * 0.67)\n",
    "        test_size = len(dataset_scaled) - train_size\n",
    "        train, test = dataset_scaled[0:train_size,:], dataset_scaled[train_size:len(x),:]\n",
    "        print(len(train), len(test))\n",
    "\n",
    "        # convert an array of values into a dataset matrix\n",
    "        def create_dataset(data, look_back=1):\n",
    "            dataX, dataY = [], []\n",
    "            i_range = len(data) - look_back - 1\n",
    "            print(i_range)\n",
    "            for i in range(0, i_range):\n",
    "                dataX.append(data[i:(i+look_back)])    # index can move down to len(dataset)-1\n",
    "                dataY.append(data[i + look_back])      # Y is the item that skips look_back number of items\n",
    "\n",
    "            return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "        # try it\n",
    "        look_back = 4\n",
    "        dataX, dataY = create_dataset(dataset_scaled, look_back=look_back)\n",
    "\n",
    "        print(\"X shape:\", dataX.shape)\n",
    "        print(\"Y shape:\", dataY.shape)\n",
    "\n",
    "        print(\"Xt-3     Xt-2      Xt-1      Xt        Y\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        for z in range(len(dataX)): \n",
    "            print('%.2f   %.2f    %.2f    %.2f    %.2f' % (dataX[z][0][0], dataX[z][1][0], dataX[z][2][0], dataX[z][3][0], dataY[z][0]))\n",
    "\n",
    "\n",
    "        # Reshape to (samples, timestep, features)\n",
    "        dataX = numpy.reshape(dataX, (dataX.shape[0], 1, dataX.shape[1]))\n",
    "\n",
    "        print(\"X shape:\", dataX.shape)\n",
    "\n",
    "        # reshape into X=t and Y=t+1\n",
    "        look_back = 1\n",
    "        trainX, trainY = create_dataset(train, look_back)      # trainX is input, trainY is expected output\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "\n",
    "        # reshape input to be [samples, time steps, features]\n",
    "        print(\"Original trainX shape:\", trainX.shape)\n",
    "        trainX = numpy.reshape(trainX, (trainX.shape[0], look_back, trainX.shape[1]))     # timestep = 1, input_dim = trainX.shape[1]\n",
    "        testX = numpy.reshape(testX, (testX.shape[0], look_back, testX.shape[1]))\n",
    "        print(\"New trainX shape:\", trainX.shape)\n",
    "        print(\"trainY shape:\", trainY.shape)\n",
    "        print(\"trainY example:\", trainY[0])\n",
    "\n",
    "\n",
    "        # create and fit the LSTM network\n",
    "        from keras.layers import Dropout\n",
    "\n",
    "        batch_size = 1\n",
    "        timesteps = trainX.shape[1]\n",
    "        input_dim = trainX.shape[2]\n",
    "\n",
    "        model = Sequential()\n",
    "        #model.add(LSTM(8, input_shape=(1, look_back)))    # 4 or 8 is the number of LSTM units or dimensions of output of LSTM layer, can be any integer. input_shape must show up in the first layer\n",
    "        model.add(LSTM(4, batch_input_shape=(batch_size, timesteps, input_dim)))\n",
    "        model.add(Dense(10))    # not necessary, but just want to try it\n",
    "        #model.add(Dropout(0.8))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=2)   # it turns out that epochs = 15 is enough, 100 is not necessary\n",
    "\n",
    "\n",
    "        # make predictions\n",
    "        trainPredict = model.predict(trainX, batch_size)\n",
    "        testPredict = model.predict(testX, batch_size)      \n",
    "\n",
    "        # invert predictions\n",
    "        trainPredict = scaler.inverse_transform(trainPredict)\n",
    "        trainY = scaler.inverse_transform(trainY)               # trainY is of shape (samples, features) while trainX is of (samples, timesteps, features) )\n",
    "        testPredict = scaler.inverse_transform(testPredict)\n",
    "        testY = scaler.inverse_transform(testY)\n",
    "\n",
    "        print(\"trainY shape:\", trainY.shape)\n",
    "        print(\"trainPredict shape:\", trainPredict.shape)\n",
    "        print(\"testY shape:\", testY.shape)\n",
    "        print(\"testPredict shape:\", testPredict.shape)\n",
    "\n",
    "        # calculate root mean squared error\n",
    "        trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "        print('Train Score: %.2f RMSE' % (trainScore))\n",
    "        testScore = math.sqrt(mean_squared_error(testY, testPredict))\n",
    "        print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "        # shift train predictions for plotting\n",
    "        trainPredictPlot = numpy.empty_like(x)\n",
    "        trainPredictPlot[:, :] = numpy.nan\n",
    "        #print(trainPredictPlot[0])\n",
    "        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "        # shift test predictions for plotting\n",
    "        testPredictPlot = numpy.empty_like(x)\n",
    "        testPredictPlot[:, :] = numpy.nan\n",
    "        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(x)-1, :] = testPredict\n",
    "\n",
    "        # plot baseline and predictions\n",
    "        plt.plot(scaler.inverse_transform(dataset_scaled))\n",
    "        #plt.plot(dataset_scaled)\n",
    "        plt.plot(trainPredictPlot)\n",
    "        plt.plot(testPredictPlot)\n",
    "        plt.show()\n",
    "\n",
    "        x = x.reset_index()\n",
    "        x[\"test\"] = testPredictPlot\n",
    "        \n",
    "        if len(x[x[\"ds\"]==\"2019-02\"][\"test\"])!=0:\n",
    "            num = int(x[x[\"ds\"]==\"2019-02\"][\"test\"])\n",
    "            brnach_code1 = i\n",
    "            product_id = y\n",
    "\n",
    "            x = df[(df[\"branch_code\"]==i)&(df[\"item_cat\"]==y)][[\"ym\",\"sales\"]].groupby(\"ym\").sum().reset_index().rename({\"ym\":\"ds\",\"sales\":\"y\"},axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            fbpred.append(num)\n",
    "            fbpred1.append(brnach_code1)\n",
    "            fbpred2.append(product_id)\n",
    "        \n",
    "\n",
    "fp[\"pred\"] = fbpred\n",
    "fp[\"branch\"] = fbpred1\n",
    "fp[\"product\"] = fbpred2\n",
    "\n",
    "\n",
    "final2 = pd.merge(final1, fp,  how='left', left_on=['branch_code','item_cat'], right_on = ['branch','product'])\n",
    "final2.groupby([\"branch_code\",\"item_cat\",\"ym\"]).sum().plot(figsize=(20,20))\n",
    "\n",
    "final3 = final2[[\"branch_code\",\"item_cat\",\"ym\",\"yhat\",\"pred_x\",\"pred_y\",\"sales\"]].groupby([\"branch_code\",\"item_cat\",\"ym\"]).sum().reset_index()\n",
    "\n",
    "final3[\"end\"] = (final3[\"yhat\"] + final3[\"pred_x\"] + final3[\"pred_y\"] )/3\n",
    "\n",
    "final3.groupby([\"branch_code\",\"item_cat\",\"ym\"]).sum()[[\"sales\",\"end\"]].plot(figsize=(20,20))\n",
    "\n",
    "print('Stacking = ',r2_score(final3[\"sales\"],final3[\"yhat\"]))\n",
    "print('FP = ',r2_score(final3[\"sales\"],final3[\"pred_x\"]))\n",
    "print('LSTM = ',r2_score(final3[\"sales\"],final3[\"pred_y\"]))\n",
    "print('AVG = ',r2_score(final3[\"sales\"],final3[\"end\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Development",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
